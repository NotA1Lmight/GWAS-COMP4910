{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADMM adaptive lasso using ProximalOperators with line and golden section search\n",
    "# Automatic tuning of learning rate and regularization parameter\n",
    "# This code only only changes the ratio between test data and train data, and changes the input file.\n",
    "# The original source code from https://github.com/patwa67/AUTALASSO.\n"
    "\n",
    "using ProximalOperators\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lasso_admm (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for Proximal ADMM lasso with line search\n",
    "function lasso_admm(Xtrainhot, ytrain, lam, theta, beta, f, abscovinv,tol; maxit=5000)\n",
    "  u = zero(theta)\n",
    "  grad = zero(theta)\n",
    "  lamw = lam*abscovinv # Regularization parameter times weights\n",
    "  g = NormL1(lamw) # Regularization function\n",
    "  c = 0.5\n",
    "  lr = 1.0\n",
    "  loss(theta) = 0.5*norm(Xtrainhot*theta-ytrain)^2 # Loss function for line search\n",
    "  for it = 1:maxit\n",
    "    # Line search\n",
    "    it % 8 == 1 && (grad = Xtrainhot'*(Xtrainhot*beta-ytrain))\n",
    "    while  it % 20 == 2 && loss(theta) > (loss(beta) + grad'*(-beta) + (1.0/(2.0*lr))*norm(-beta)^2)\n",
    "      lr = lr * c\n",
    "      #println(lr)\n",
    "    end\n",
    "    gam = lr\n",
    "    # ADMM perform f-update step\n",
    "    prox!(beta, f, theta - u, gam)\n",
    "    # ADMM perform g-update step\n",
    "    prox!(theta, g, beta + u, gam)\n",
    "    # Stopping criterion for ADMMM\n",
    "    if norm(beta-theta, Inf) <= tol*(1+norm(u, Inf))\n",
    "      break\n",
    "    end\n",
    "    # Dual update\n",
    "    u .+= beta - theta\n",
    "    #print(it)\n",
    "  end\n",
    "  return theta,beta,tol\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gss_opt (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function for Golden section search to optimize lambda\n",
    "function gss_opt(alam, blam, tolgss, Xtesthot, ytest,abscovinv,maxnorm)\n",
    "  lama =alam*maxnorm # Lower lambda\n",
    "  lamb =blam*maxnorm # Higher lambda\n",
    "  gr = (sqrt(5.0) + 1.0) / 2.0 # Golden section ratio\n",
    "  toladmm = 1e-4 # Convergence tolerance for ADMM\n",
    "  fc = lasso_admm(Xtrainhot, ytrain, lama, zero(Xtrainhot[1,:]),zero(Xtrainhot[1,:]),f, abscovinv,toladmm)\n",
    "  lossc= 0.5*norm(Xtesthot*fc[1].-ytest)^2 # Test error for initial lower lambda\n",
    "  fd = lasso_admm(Xtrainhot, ytrain, lamb, zero(Xtrainhot[1,:]),zero(Xtrainhot[1,:]),f, abscovinv,toladmm)\n",
    "  lossd= 0.5*norm(Xtesthot*fd[1].-ytest)^2 # Test error for initial higher lambda\n",
    "  iter = 2\n",
    "  meanlam = zero(1.0:100.0)\n",
    "  #meanlam[iter] = (lama+lamb)/2\n",
    "  meanloss = zero(1.0:100.0)\n",
    "  #meanloss[1] = max(lossc,lossd)\n",
    "  #meanloss[iter] = (lossc+lossd)/2\n",
    "  lamc = lamb - (lamb - lama) / gr\n",
    "  lamd = lama + (lamb - lama) / gr\n",
    "  println(\"lossc =$lossc\")\n",
    "  println(\"lossd =$lossd\")\n",
    "  println(\"lambdaa =$lama\")\n",
    "  println(\"lambdab =$lamb\")\n",
    "  println(\"lambdac =$lamc\")\n",
    "  println(\"lambdad =$lamd\")\n",
    "  iter = 2\n",
    "  nodrun = 0\n",
    "  while abs(lamc - lamd)/((lamc + lamd)/2.0) > tolgss # Run GSS until convergence\n",
    "    iter = iter+1\n",
    "    if iter == 3\n",
    "    fc = lasso_admm(Xtrainhot, ytrain, lamc, fc[1],fc[2],f, abscovinv,toladmm)\n",
    "    lossc= 0.5*norm(Xtesthot*fc[1].-ytest)^2 # Test error for initial lower lambda\n",
    "    fd = lasso_admm(Xtrainhot, ytrain, lamd, fd[1],fd[2],f, abscovinv,toladmm)\n",
    "    lossd= 0.5*norm(Xtesthot*fd[1].-ytest)^2 # Test error for initial higher lambda\n",
    "    else\n",
    "    if nodrun==1\n",
    "    fc = lasso_admm(Xtrainhot, ytrain, lamc, fc[1],fc[2],f, abscovinv,toladmm)\n",
    "    lossc= 0.5*norm(Xtesthot*fc[1].-ytest)^2 # Test error for initial lower lambda\n",
    "    else\n",
    "    fd = lasso_admm(Xtrainhot, ytrain, lamd, fd[1],fd[2],f, abscovinv,toladmm)\n",
    "    lossd= 0.5*norm(Xtesthot*fd[1].-ytest)^2 # Test error for initial higher lambda\n",
    "    end\n",
    "    end\n",
    "    meanlam[iter] = (lamc+lamd)/2.0\n",
    "    meanloss[iter] = (lossc+lossd)/2.0\n",
    "    # Stop GSS if test MSE is increased two consecutive iterations\n",
    "    if (meanloss[iter] > meanloss[iter-1])&&(meanloss[iter-1] > meanloss[iter-2])\n",
    "      break\n",
    "    end\n",
    "    if lossc < lossd\n",
    "      lamb = lamd\n",
    "      fd=fc\n",
    "      lossd=lossc\n",
    "      nodrun=1\n",
    "    else\n",
    "      lama = lamc\n",
    "      fc=fd\n",
    "      lossc=lossd\n",
    "      nodrun=0\n",
    "    end\n",
    "    lamc = lamb - (lamb - lama) / gr\n",
    "    lamd = lama + (lamb - lama) / gr\n",
    "    #println(\"lossc =$lossc\")\n",
    "    #println(\"lossd =$lossd\")\n",
    "    println(\"lambdac =$lamc\")\n",
    "    println(\"lambdad =$lamd\")\n",
    "  end\n",
    "  # Final ADMM for optimized lambda\n",
    "  lamopt = meanlam[iter-2]\n",
    "  fmean1 = (fc[1]+fd[1])/2.0\n",
    "  fmean2 = (fc[2]+fd[2])/2.0\n",
    "  fopt = lasso_admm(Xtrainhot, ytrain, lamopt, fmean1,fmean2,f, abscovinv,toladmm)\n",
    "  lossopt= 0.5*norm(Xtesthot*fopt[1].-ytest)^2\n",
    "  acc = cor(Xtesthot*fopt[1],ytest)\n",
    "\n",
    "  return lamopt,fopt,lossopt,acc\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140×214051 Array{Float64,2}:\n",
       " 0.0  0.0  0.0  2.0  2.0  0.0  0.0  2.0  …  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     0.0  2.0  0.0  0.0  2.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  2.0     0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 0.0  2.0  0.0  0.0  0.0  2.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0  …  2.0  2.0  0.0  0.0  2.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0     2.0  2.0  0.0  0.0  2.0  0.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     2.0  2.0  0.0  0.0  2.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  2.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     0.0  2.0  2.0  0.0  2.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  0.0  2.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " ⋮                        ⋮              ⋱       ⋮                        ⋮\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  2.0  0.0  0.0  2.0  2.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0  …  0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  2.0\n",
       " 2.0  2.0  2.0  0.0  0.0  0.0  0.0  2.0     0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0  …  0.0  0.0  0.0  0.0  0.0  2.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  2.0  0.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  2.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0     0.0  2.0  0.0  0.0  2.0  0.0  0.0\n",
       " 0.0  0.0  0.0  0.0  0.0  0.0  2.0  0.0     0.0  0.0  0.0  0.0  0.0  0.0  2.0\n",
       " 0.0  0.0  0.0  2.0  0.0  0.0  0.0  2.0     0.0  2.0  0.0  0.0  2.0  0.0  0.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read QTLMAS2010 data, and partition into train and test parts\n",
    "# (given that the data file is in the working directory)\n",
    "X = readdlm(\"_p1.csv\",',')\n",
    "ytot = (X[:,1].-mean(X[:,1])) # Center y to mean zero\n",
    "ytrain = ytot[1:140]\n",
    "Xtest= X[141:size(X)[1],2:size(X)[2]]\n",
    "ytest = ytot[141:size(X)[1]]\n",
    "Xtrain = X[1:140,2:size(X)[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding training data\n",
    "Xtrain0 = copy(Xtrain)\n",
    "Xtrain1 = copy(Xtrain)\n",
    "Xtrain2 = copy(Xtrain)\n",
    "Xtrain0[Xtrain0.==1] .= 2\n",
    "Xtrain0[Xtrain0.==0] .= 1\n",
    "Xtrain0[Xtrain0.==2] .= 0\n",
    "Xtrain1[Xtrain1.==2] .= 0\n",
    "Xtrain2[Xtrain2.==1] .= 0\n",
    "Xtrain2[Xtrain2.==2] .= 1\n",
    "Xtrainhot = hcat(Xtrain0,Xtrain1,Xtrain2)\n",
    "# Set unimportant allocations to zero\n",
    "Xtrain0 = 0\n",
    "Xtrain1 = 0\n",
    "Xtrain2 = 0\n",
    "Xtrain = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One hot encoding test data\n",
    "Xtest0 = copy(Xtest)\n",
    "Xtest1 = copy(Xtest)\n",
    "Xtest2 = copy(Xtest)\n",
    "Xtest0[Xtest0.==1] .= 2\n",
    "Xtest0[Xtest0.==0] .= 1\n",
    "Xtest0[Xtest0.==2] .= 0\n",
    "Xtest1[Xtest1.==2] .= 0\n",
    "Xtest2[Xtest2.==1] .= 0\n",
    "Xtest2[Xtest2.==2] .= 1\n",
    "Xtesthot = hcat(Xtest0,Xtest1,Xtest2)\n",
    "# Set unimportant allocations to zero\n",
    "Xtest0 = 0\n",
    "Xtest1 = 0\n",
    "Xtest2 = 0\n",
    "Xtest = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642153×1 Array{Float64,2}:\n",
       "  0.4908749227490003\n",
       "  0.4771888525152953\n",
       "  0.406929937371266\n",
       "  1.5005011951576834\n",
       "  4.788975021533163\n",
       "  3.159603831790878\n",
       "  5.942892044586966\n",
       "  0.7627334548376351\n",
       "  0.4357074088171417\n",
       "  1.343134209890603\n",
       "  5.401804302567662\n",
       "  0.4087977648467534\n",
       "  2.843781966973549\n",
       "  ⋮\n",
       "  0.32814528775947244\n",
       "  1.3653265979092122\n",
       " 14.000000000000012\n",
       "  0.7919744419347619\n",
       "  3.4248504047870467\n",
       "  0.8861162970720823\n",
       "  0.37022592152199757\n",
       "  0.8449297700974753\n",
       "  1.5536926147704588\n",
       "  0.3333076415829544\n",
       "  1.072826506422625\n",
       "  1.070613154347647"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Factor for initial lower lambda\n",
    "alam = 0.0001\n",
    "# Factor for initial upper lambda\n",
    "blam = 1.0\n",
    "# Convergence factor for lambda in gss_opt\n",
    "tolgss = 0.01\n",
    "# Find lambda where all reg coeff are zero\n",
    "maxnorm = norm(Xtrainhot'*ytrain, Inf)\n",
    "# The least squares loss function\n",
    "f = LeastSquares(Xtrainhot, ytrain)\n",
    "# Inverse covariances to be used as weights in the adaptive lasso\n",
    "abscovinv = 1.0./abs.(cov(Xtrainhot,ytrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossc =4229.465064522015\n",
      "lossd =5447.931539558228\n",
      "lambdaa =0.06713917525773198\n",
      "lambdab =671.3917525773198\n",
      "lambdac =256.49032401046225\n",
      "lambdad =414.96856774211534\n",
      "lambdac =158.5453829069108\n",
      "lambdad =256.4903240104623\n",
      "lambdac =98.0120802788093\n",
      "lambdad =158.54538290691073\n",
      "lambdac =60.6004418033592\n",
      "lambdad =98.01208027880926\n",
      "lambdac =98.01208027880926\n",
      "lambdad =121.13374443146068\n",
      "181.505267 seconds (4.56 M allocations: 40.129 GiB, 1.66% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(128.27873159286003, ([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0  …  0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.9953104863089663e-6, -9.959809543103049e-6, 2.1437474743146723e-6, -2.8911021961869343e-6, -1.672410585769335e-5, -4.7830416204944215e-5, -3.0690124263044183e-6, 2.3067393547099302e-5, 2.87291410200341e-6, 1.5244717047335765e-6  …  -2.320921021559813e-5, 2.410055990303972e-5, 6.293371769501566e-6, -1.657054821774251e-6, 6.764183067997642e-6, 2.0141317963240368e-5, 3.8654173095953315e-6, 8.292397206954849e-6, -8.2771780253732e-6, 1.3737009362857489e-5], 0.0001), 4268.057561043991, 0.7603619583796325)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run AUTALASSO with timing\n",
    "@time res = gss_opt(alam, blam, tolgss, Xtesthot, ytest,abscovinv,maxnorm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save regression coefficients, lambda, MSE and ACC to text files\n",
    "writedlm(\"outbetaQTLMAS.txt\", res[2][1])\n",
    "writedlm(\"outlambdaQTLMAS.txt\", res[1])\n",
    "writedlm(\"outMSEQTLMAS.txt\", res[3]/length(ytest)*2)\n",
    "writedlm(\"outACCQTLMAS.txt\", res[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54-element Array{Float64,1}:\n",
       " -10.404807245066815\n",
       "  -7.752280837514913\n",
       "  -8.815209609131697\n",
       "   0.8276948409987757\n",
       "  33.96027414976013\n",
       "   1.8881055018222694\n",
       " -14.101722812384395\n",
       "  -1.6821627111589013\n",
       "   6.204319260828259\n",
       "   6.833472981465372\n",
       "  -6.397185037559083\n",
       "  17.644217417549918\n",
       "  -3.8980304881991197\n",
       "   ⋮\n",
       "  27.10557551506976\n",
       "   4.21308996841493\n",
       "  -6.688843635065385\n",
       "   5.032919630772658\n",
       "  -8.161652405286027\n",
       "   4.777885759693488\n",
       "  -8.919706304328809\n",
       " -17.307410473627513\n",
       "   0.4564851753545702\n",
       "  -2.069493951837476\n",
       " -15.219667243967297\n",
       "  -8.391363577641963"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict observations for test data\n",
    "ytesthat = Xtesthot*res[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
